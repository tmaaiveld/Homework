{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import random\n",
    "import copy\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Tommy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in sqrt\n",
      "C:\\Users\\Tommy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Tommy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "### ===================       2.1 UCB versus epsilon-greedy     =================== ###### \n",
    "    \n",
    "exploration_approaches = ['epsilon-greedy', 'UCB', 'greedy_optimistic_init'] #  \n",
    "timesteps = 300\n",
    "iterations = 1000\n",
    "k = 5 # Nr of bandits (or actions). Could do more\n",
    "actions = list(range(0, k))\n",
    "chosen_actions = []\n",
    "means = [1,2,3,4,5] # mean reward for each arm\n",
    "sd = 1 # standard deviation\n",
    "\n",
    "epsilon = 0.1\n",
    "c = 2 # For UCB. Could tune this  \n",
    "\n",
    "\n",
    "def greedy_selection(Q, epsilon): # function now handles optimistic greedy for epsilon = 0\n",
    "    all_possible_actions = copy.copy(actions)\n",
    "    greedy_action = shuffle_argmax(Q)\n",
    "    all_possible_actions.remove(greedy_action)\n",
    "    exploratory_action = random.choices(all_possible_actions)[0] # need to index else we get []s\n",
    "    chosen_action = exploratory_action if random.uniform(0,1) < epsilon else greedy_action\n",
    "    return chosen_action\n",
    "\n",
    "def shuffle_argmax(Q):\n",
    "    ind = list(range(0,len(Q)))\n",
    "    random.shuffle(ind)\n",
    "    Q_shuf = [Q[i] for i in ind]\n",
    "    best = np.argmax(Q_shuf)\n",
    "    return ind[best]\n",
    "    \n",
    "def pick_best_accordingto_UCB(Q):\n",
    "    Q_UCB = [0] * len(Q) # initializing\n",
    "    for action, action_value in enumerate(Q):              # should be a loop-less way, but this works for now\n",
    "        UCB = c * np.sqrt(np.log(t)/(2*selection_history[action]))\n",
    "        Q_UCB[action] = action_value + UCB\n",
    "    return shuffle_argmax(Q_UCB)         \n",
    "    \n",
    "def select_action(Q, exploration_approach):  \n",
    "    if exploration_approach == 'epsilon-greedy':\n",
    "        chosen_action = greedy_selection(Q, 0.1)\n",
    "    elif exploration_approach == 'UCB':\n",
    "        chosen_action = pick_best_accordingto_UCB(Q)\n",
    "    elif exploration_approach == 'greedy_optimistic_init':\n",
    "        chosen_action = greedy_selection(Q, 0)\n",
    "    return chosen_action\n",
    "            \n",
    "    \n",
    "############              MAIN               ##############\n",
    "    \n",
    "for nth_strategy, strategy in enumerate(exploration_approaches):\n",
    " \n",
    "    average_rewards = [0] * timesteps # initialize a list that will keep track of (incrementally updated) average reward at each t over all iterations\n",
    "    \n",
    "    opt_action_count = [0] * timesteps\n",
    "    for i in list(range(iterations)):\n",
    "        \n",
    "        Q = [0] * k if strategy != 'greedy_optimistic_init' else [10] * k    # Initializing action values\n",
    "        selection_history = [0] * k   # initialize an array that will record how often actions have been selected\n",
    " \n",
    "        for t in list(range(timesteps)): \n",
    "            a = select_action(Q, strategy) # which lever will we pull under the given strategy\n",
    "            reward = np.random.normal(means[a], sd)     \n",
    "            \n",
    "            average_rewards[t] += (reward - average_rewards[t]) / (i+1) # i+1, otherwise we'd divide by 0 in the 0-th iteration    \n",
    "            \n",
    "            selection_history[a] += 1\n",
    "            Q[a] = Q[a] + (1/(selection_history[a]+1)) * (reward - Q[a])\n",
    "            if a == 4:\n",
    "                opt_action_count[t] += 1\n",
    "        \n",
    "    plt.plot(list(range(len(average_rewards))), average_rewards, label=strategy)\n",
    "#     plt.plot(list(range(len(opt_action_count))),[count / iterations for count in opt_action_count], label=strategy)\n",
    "\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('average reward')\n",
    "# plt.ylabel('% Optimal action')\n",
    "plt.legend()\n",
    "plt.savefig('assignment1_2-1.eps')\n",
    "# plt.savefig('assignment1_2-1_opt.eps')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
